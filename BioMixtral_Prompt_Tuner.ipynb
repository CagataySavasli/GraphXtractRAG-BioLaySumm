{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-14T11:28:55.584117Z",
     "start_time": "2024-12-14T11:28:52.643506Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import textstat\n",
    "\n",
    "from src.RAG_Calculater import RAG, get_top_n_articles\n",
    "from src.Prompt_Factory import prompt_factory\n",
    "from src.Case_Builder import (bert_version,\n",
    "                              genai_version,\n",
    "                              genai_model_name,\n",
    "                              dataset_name,\n",
    "                              massage_strategy\n",
    "                              )"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:29:05.842231Z",
     "start_time": "2024-12-14T11:28:55.585099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_train = pd.read_json(f'src/dataset/clean/{dataset_name}/{bert_version}_train.json')\n",
    "data_val = pd.read_json(f'src/dataset/clean/{dataset_name}/{bert_version}_validation.json')"
   ],
   "id": "13a3de3f31a75dd7",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:29:06.988080Z",
     "start_time": "2024-12-14T11:29:05.855163Z"
    }
   },
   "cell_type": "code",
   "source": "data_val = data_val.loc[:4].copy()",
   "id": "ee69879d787d7db",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T11:29:08.247570Z",
     "start_time": "2024-12-14T11:29:06.991270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_train['rag_sentences'] = data_train['sentences_similarity'].apply(RAG)\n",
    "data_val['rag_sentences'] = data_val['sentences_similarity'].apply(RAG)"
   ],
   "id": "b66bbda47b7c2d8a",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "chatbot = pipeline(\"text-generation\", model=genai_model_name, min_new_tokens=256, max_new_tokens=512, repetition_penalty=1.2, no_repeat_ngram_size=3)",
   "id": "dc84f9eeebab13e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ROUGE and BERTScore\n",
    "rouge = evaluate.load('rouge')\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Load a pre-trained BART model for BARTScore\n",
    "bart_model_name = \"facebook/bart-large-cnn\"\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(bart_model_name)\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(bart_model_name)\n",
    "\n",
    "def get_results(results):\n",
    "    batch_size = 5\n",
    "    \n",
    "    # Compute ROUGE metrics\n",
    "    print(\"ROUGE Metrics Calculater:\")\n",
    "    rouge_results = rouge.compute(\n",
    "        predictions=results['prediction'],\n",
    "        references=results['reference'],\n",
    "        use_aggregator=True,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "    # Compute BERTScore\n",
    "    print(\"BERTScore Calculater:\")\n",
    "    bertscore_results = {\n",
    "        \"precision\": [],\n",
    "        \"recall\": [],\n",
    "        \"f1\": [],\n",
    "    }\n",
    "    for idx in tqdm(range(0, len(results), batch_size)):\n",
    "        str_idx = idx\n",
    "        end_idx = idx + batch_size\n",
    "        tmp_bertscore_results = bertscore.compute(\n",
    "            predictions=results['prediction'][str_idx:end_idx].to_list(),\n",
    "            references=results['reference'][str_idx:end_idx].to_list(),\n",
    "            model_type=\"microsoft/deberta-xlarge-mnli\",\n",
    "        )\n",
    "        bertscore_results[\"precision\"].extend(tmp_bertscore_results[\"precision\"])\n",
    "        bertscore_results[\"recall\"].extend(tmp_bertscore_results[\"recall\"])\n",
    "        bertscore_results[\"f1\"].extend(tmp_bertscore_results[\"f1\"])\n",
    "    \n",
    "    # Compute FKGL and DCRS for Readability\n",
    "    print(\"FKGL Metrics Calculater:\")\n",
    "    fkgl_scores = [textstat.flesch_kincaid_grade(p) for p in results['prediction'].to_list()]\n",
    "    print(\"DCRS Metrics Calculater:\")\n",
    "    dcrs_scores = [textstat.dale_chall_readability_score(p) for p in results['prediction'].to_list()]\n",
    "    \n",
    "    # Compute BARTScore for Factuality\n",
    "    def compute_bart_score(predictions, references):\n",
    "        bart_scores = []\n",
    "        for pred, ref in zip(predictions, references):\n",
    "            inputs = bart_tokenizer(ref, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "            outputs = bart_tokenizer(pred, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "            ref_to_pred_score = bart_model(**inputs, labels=outputs[\"input_ids\"]).loss.item()\n",
    "            pred_to_ref_score = bart_model(**outputs, labels=inputs[\"input_ids\"]).loss.item()\n",
    "            bart_scores.append((ref_to_pred_score + pred_to_ref_score) / 2)\n",
    "        return bart_scores\n",
    "    \n",
    "    print(\"BARTScore Calculater:\")\n",
    "    bart_scores = {\n",
    "        \"bart_scores\": [],\n",
    "    }\n",
    "    for idx in tqdm(range(0, len(results), batch_size)):\n",
    "        str_idx = idx\n",
    "        end_idx = idx + batch_size\n",
    "        tmp_bart_scores = compute_bart_score(results['prediction'][str_idx:end_idx].to_list(), results['reference'][str_idx:end_idx].to_list())\n",
    "        bart_scores[\"bart_scores\"].extend(tmp_bart_scores)\n",
    "        \n",
    "    final_results = {\n",
    "        \"ROUGE1\": [rouge_results['rouge1']],\n",
    "        \"ROUGE2\": [rouge_results['rouge2']],\n",
    "        \"ROUGEL\": [rouge_results['rougeL']],\n",
    "        \"BERTScore_Precision\": [np.average(bertscore_results[\"precision\"])],\n",
    "        \"BERTScore_Recall\": [np.average(bertscore_results[\"recall\"])],\n",
    "        \"BERTScore_F1\": [np.average(bertscore_results[\"f1\"])],\n",
    "        \"FKGL\": [np.average(fkgl_scores)],\n",
    "        \"DCRS\": [np.average(dcrs_scores)],\n",
    "        \"BARTScore\": [np.average(bart_scores[\"bart_scores\"])],\n",
    "    }\n",
    "    \n",
    "    result_df = pd.DataFrame(final_results)\n",
    "    \n",
    "    return result_df"
   ],
   "id": "5bad78a0744afb99",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_predictions(prompt_strategy_used):\n",
    "    results = []\n",
    "    summaries = []\n",
    "    \n",
    "    for idx in range(len(data_val)):\n",
    "        print(f\"\\n {idx+1} / {len(data_val)}\", end=\"\")\n",
    "        \n",
    "        target_row = data_val.loc[idx]\n",
    "        \n",
    "        if massage_strategy == \"few_shot\": \n",
    "            ref_rows_indexes = get_top_n_articles(data_train['title_embedding'], target_row['title_embedding'], n=3)\n",
    "            ref_rows = data_train.loc[ref_rows_indexes].reset_index(drop=True)\n",
    "            \n",
    "        else: \n",
    "            ref_rows = None\n",
    "        \n",
    "        prompt = prompt_factory(prompt_strategy_used, target_row, ref_rows)\n",
    "        massage = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        summary = \" \".join(data_val.loc[idx, 'summary'])\n",
    "        \n",
    "        answer = chatbot(massage)\n",
    "        \n",
    "        if genai_version == \"BioGBT\":\n",
    "            answer = answer[0]['generated_text'].split(\"## Answer:\\n\")[-1]\n",
    "        else:\n",
    "            answer = answer[0]['generated_text'][-1]['content']\n",
    "            \n",
    "        results.append(answer)\n",
    "        summaries.append(summary)\n",
    "    \n",
    "    model_results = pd.DataFrame({\n",
    "        'reference': summaries,\n",
    "        'prediction': results\n",
    "    })\n",
    "    \n",
    "    return model_results"
   ],
   "id": "4120dfcd0b06798d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for prompt_strategy_used in [1,2]:\n",
    "    model_results = get_predictions(prompt_strategy_used)\n",
    "    result_df = get_results(model_results)\n",
    "    \n",
    "    model_results.to_csv(f'results/{genai_version}_summaries_{bert_version}_{dataset_name}_{prompt_strategy_used}_val.csv', index=False)\n",
    "    \n",
    "    result_df.to_csv(f'results/{genai_version}_results_{bert_version}_{dataset_name}_{prompt_strategy_used}_val.csv', index=False)"
   ],
   "id": "a8b93f5d05a6501a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "35e7c26b3967983"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
