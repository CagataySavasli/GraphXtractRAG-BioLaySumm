# ----------------------------------------------------------------------------
# This file contains the SelectorPipelineGYM class, which is a class that
# is used to train selector models in pipeline stategy.
# ----------------------------------------------------------------------------

from lib.selectors.GCNSelector import GCNSelector
from lib.selectors.GATSelector import GATSelector
from lib.selectors.MIXSelector import MIXSelector
from lib.prompt_factories.PromptFactory import PromptFactory
from lib.utility.CaseBuilder import CaseBuilder
from lib.utility.ResultCalculator import ResultCalculator
from lib.utility.GraphGenerator import GraphGenerator

from typing import List, Tuple
import time

import pandas as pd
import numpy as np

from sklearn.metrics.pairwise import cosine_similarity
from rouge_score import rouge_scorer

import matplotlib.pyplot as plt

import torch
import torch.optim as optim
import torch.nn.functional as F
from torch_geometric.data import Data

import google.generativeai as genai

import os

import io
import warnings
import contextlib
import logging

logging.getLogger("transformers").setLevel(logging.ERROR)

class SelectorPipelineGYM:
    def __init__(self, selector_type: str,
                 df_train: pd.DataFrame,
                 df_test: pd.DataFrame):

        # Initialize the case builder
        self.case_builder = CaseBuilder()

        # Initialize the dataframes
        self.df_train = df_train.copy()
        self.df_test = df_test.copy()

        # Initialize the selector model
        if selector_type == "GCN":
            self.selector = GCNSelector(in_channels=770, hidden_channels=128)
        elif selector_type == "GAT":
            self.selector = GATSelector(in_channels=770, hidden_channels=128)
        elif selector_type == "MIX":
            self.selector = MIXSelector(in_channels=770, hidden_channels=128)
        else:
            raise ValueError("Unsupported selector type")

        # Number of nodes to select
        self.n_select = self.case_builder.rag_n

        self.selector_path = f"./outputs/models/Grap_{self.case_builder.dataset_name}_{self.n_select}_selector.pth"

        # Initialize the optimizer
        self.optimizer = optim.Adam(self.selector.parameters(), lr=self.case_builder.lr)

        # Global baseline variables for Exponential Moving Average (EMA)
        self.running_reward_baseline = 0.0  # Initial baseline value
        self.reward_smoothing_factor = 1.0  # Decay rate for EMA

        # Train loss history for analysis train performance
        self.train_loss_history = []

        # Reward Calculater
        self.reward_calculater = ResultCalculator()

        # Graph Generator
        self.graph_generator = GraphGenerator()

        # For prevent useless print:
        self.f = io.StringIO()

    def compute_rl_loss(self, log_probabilities: torch.Tensor,
                        generated_summary: str,
                        ground_truth_summary: str) -> torch.Tensor:
        """
        Computes the REINFORCE-style loss function using an Exponential Moving Average (EMA) baseline.

        This function applies policy gradient loss to train the model, adjusting for variance
        using a running baseline.

        Args:
            log_probabilities (torch.Tensor): The sum of log probabilities of the selected actions.
            generated_summary (str): The summary generated by the model.
            ground_truth_summary (str): The reference summary (ground truth).

        Returns:
            torch.Tensor: The computed reinforcement learning loss.
        """


        # Compute reward score based on the similarity of generated and reference summaries
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            with contextlib.redirect_stdout(self.f), contextlib.redirect_stderr(self.f):
                reward_score = self.reward_calculater.reward_function(generated_summary, ground_truth_summary)

        # Update the baseline using Exponential Moving Average (EMA)
        self.running_reward_baseline = (
                self.reward_smoothing_factor * self.running_reward_baseline +
                (1 - self.reward_smoothing_factor) * reward_score
        )

        # Compute the advantage by subtracting the running baseline from the current reward
        advantage = reward_score - self.running_reward_baseline

        # Compute policy gradient loss (REINFORCE algorithm)
        rl_loss = -log_probabilities * advantage

        return rl_loss

    def get_graphs(self, row: pd.Series) -> Tuple[Data, List[str]]:
        self.graph_generator.set_row(row)
        data = self.graph_generator.create_graph()
        sentences = self.graph_generator.get_sentences()

        return data, sentences


    def get_n_sentences(self, row: pd.Series) -> Tuple[List[str], torch.Tensor]:

        data, sentences = self.get_graphs(row)

        # Forward pass: get logits per node
        logits = self.selector(data.x, data.edge_index)

        # Convert logits to probabilities with softmax.
        probs = F.softmax(logits, dim=0)

        # Sample n_select unique nodes based on the probability distribution.
        node_indices = torch.topk(probs, self.n_select).indices

        # Sum the log probabilities of the selected nodes.
        # (This is our “policy log–probability”.)
        selected_log_probs = torch.log(probs[node_indices]).sum()

        # Build the prompt by concatenating the sentences for the selected nodes.
        selected_sentences = [sentences[i] for i in node_indices.tolist()]

        return selected_sentences, selected_log_probs

    def merge_sentences(self, selected_sentences: List[str]) -> str:
        text = " ".join(selected_sentences)
        return text

    def train_step(self, row: pd.Series) -> torch.Tensor:
        """
        Perform a single training step for the selector model using the REINFORCE algorithm.

        Args:
            row (pd.Series): A row of dataframe containing the data needed for training.

        Returns:
            torch.Tensor: Loss function results.
        """

        self.selector.train()

        reference_summary = row['summary']

        selected_sentences, selected_log_probs = self.get_n_sentences(row)
        generated_summary = self.merge_sentences(selected_sentences)

        # Compute the loss using the REINFORCE algorithm if a summary is generated successfully.
        loss = self.compute_rl_loss(selected_log_probs, generated_summary, reference_summary)
        return loss

    def test_step(self, row: pd.Series) -> Tuple[torch.Tensor, str, str]:
        """
        Perform a single testing step for the selector model using the REINFORCE algorithm.

        Args:
            row (pd.Series): A row of dataframe containing the data needed for testing.

        Returns:
            Tuple[torch.Tensor, str]: Loss function results and the generated summary.
        """

        self.selector.eval()

        with torch.no_grad():
            reference_summary = row['summary']

            selected_sentences, selected_log_probs = self.get_n_sentences(row)
            generated_summary = self.merge_sentences(selected_sentences)

            # Compute the loss using the REINFORCE algorithm if a summary is generated successfully.
            loss = self.compute_rl_loss(selected_log_probs, generated_summary, reference_summary)


        return loss, generated_summary, reference_summary


    def train(self, n_epochs: int):
        """
        Train the selector model using the REINFORCE algorithm.

        Args:
            n_epochs (int): Number of training epochs.
        """

        for epoch in range(n_epochs):
            print(f"Epoch {epoch + 1}/{n_epochs}")
            train_loss = 0.0
            num_success = 0

            batch_loss = 0.0
            batch_success = 0

            for idx, row in self.df_train.iterrows():
                step_loss = self.train_step(row)
                train_loss += step_loss
                batch_loss += step_loss

                if step_loss > 0:
                    num_success += 1
                    batch_success += 1
                if (idx + 1) % self.case_builder.batch_size == 0 or idx == len(self.df_train) - 1:
                    batch_loss /= batch_success

                    self.optimizer.zero_grad()
                    batch_loss.backward()
                    self.optimizer.step()


                    print(
                        f"\rEpoch {epoch + 1}/{n_epochs} | Process {idx + 1}/{len(self.df_train)} - {round(((idx + 1) / len(self.df_train) * 100), 2)}% | Loss: {batch_loss:.4f}", end=" "
                        )

                    batch_loss = 0.0
                    batch_success = 0
            avg_train_loss = train_loss / num_success
            self.train_loss_history.append(avg_train_loss)
            print(f"\nEpoch {epoch + 1} Completed| Avg Train Loss: {avg_train_loss}")

        print("Training completed.")

    def test(self):
        """
        Test the selector model using the REINFORCE algorithm.
        """
        test_loss = 0.0
        predicted_summaries = []
        referance_summaries = []

        for idx, row in self.df_test.iterrows():
            loss, predicted_summary, referance_summary = self.test_step(row)
            test_loss += loss
            predicted_summaries.append(predicted_summary)
            referance_summaries.append(referance_summary)
            print(
                f"Process {idx + 1}/{len(self.df_test)} - {round(((idx + 1) / len(self.df_test) * 100), 2)}% | Test Loss: {loss}",
                )

        print(f"Test Loss: {test_loss}")
        return predicted_summaries, referance_summaries

    def save_selector(self):
        """
        Save the trained selector model.
        """
        torch.save(self.selector.state_dict(), self.selector_path)
        print("Selector model saved.")

    def load_selector(self):
        """
        Load the trained selector model.
        """
        self.selector.load_state_dict(torch.load(self.selector_path, weights_only=True))
        print("Selector model loaded.")

    def plot_training_loss(self):
        print("Plot training loss is saved.")
        plt.figure(figsize=(8, 5))
        loss_history = [loss.detach().numpy() for loss in self.train_loss_history]
        plt.plot(range(1, len(loss_history) + 1), loss_history, marker='o', linestyle='-')
        plt.xlabel('Epoch')
        plt.ylabel('Average Training Loss')
        plt.title('Training Loss Over Epochs')
        plt.grid()
        #plt.show()
        plt.savefig(f'outputs/plots/{self.case_builder.rag_strategy}_{self.case_builder.rag_n}_{self.case_builder.dataset_name}_loss.png')
