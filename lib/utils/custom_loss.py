import torch
import torch.nn as nn
from rouge_score import rouge_scorer
import numpy as np


class RougeRewardLoss(nn.Module):
    """
    Computes a Policy Gradient Loss based on ROUGE rewards using the google-research/rouge-score library.
    Reward is calculated as the average F-measure of ROUGE-1, ROUGE-2, and ROUGE-Lsum.
    """

    def __init__(self):
        super(RougeRewardLoss, self).__init__()
        # RougeScorer'ı belirtilen metriklerle başlatıyoruz
        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeLsum'], use_stemmer=True)

    def get_reward(self, candidate_text, reference_text):
        """
        Calculates the AvgRouge reward.
        Args:
            candidate_text (str): The text generated by the LLM.
            reference_text (str): The ground truth summary.
        Returns:
            float: The average of ROUGE-1, ROUGE-2, and ROUGE-Lsum F-scores.
        """
        if not candidate_text or not reference_text:
            return 0.0

        # rouge_score kütüphanesi score(target, prediction) formatını kullanır.
        # target = reference (ground truth), prediction = candidate (generated)
        try:
            scores = self.scorer.score(reference_text, candidate_text)

            # F-Measure değerlerini alıyoruz
            r1 = scores['rouge1'].fmeasure
            r2 = scores['rouge2'].fmeasure
            rl = scores['rougeLsum'].fmeasure

            # AvgRouge Hesaplama
            avg_rouge = (r1 + r2 + rl) / 3.0
            return avg_rouge
        except Exception as e:
            print(f"Rouge calculation error: {e}")
            return 0.0

    def forward(self, log_probs, rewards):
        """
        Calculates Policy Gradient Loss: Loss = - mean(log_prob * reward)
        """
        # Ensure rewards is a tensor
        if not isinstance(rewards, torch.Tensor):
            rewards = torch.tensor(rewards, device=log_probs.device)

        # Policy Gradient: Maximize Reward -> Minimize -Reward
        # Log olasılıklar negatiftir, reward pozitiftir.
        # Loss minimize edildiğinde, yüksek reward alan log_prob'lar (yani seçilen cümleler) artırılır.
        loss = -torch.mean(log_probs * rewards)
        return loss