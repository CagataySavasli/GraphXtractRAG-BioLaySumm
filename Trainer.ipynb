{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:31.725779Z",
     "start_time": "2024-12-03T14:11:25.105078Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from src.RAG_Calculater import RAG\n",
    "from src.Prompt_Factory import prompt_factory\n",
    "from src.Case_Builder import (device,\n",
    "                              bert_version,\n",
    "                              bert_model_name,\n",
    "                              genai_model_name,\n",
    "                              prompt_strategy_used,\n",
    "                              dataset_name\n",
    "                              )"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:40.896992Z",
     "start_time": "2024-12-03T14:11:31.727039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "from rouge_score import rouge_scorer\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# BARTScore için model ve tokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# ROUGE metriğini yükle\n",
    "rouge_metric = evaluate.load('rouge')\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"\n",
    "    Computes metrics for summarization tasks using ROUGE, FKGL, DCRS, and BARTScore.\n",
    "\n",
    "    Parameters:\n",
    "    - pred: A named tuple containing predictions and labels from HuggingFace Trainer.\n",
    "\n",
    "    Returns:\n",
    "    A dictionary containing evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Tahmin ve etiketleri ayıkla\n",
    "    predictions = pred.predictions\n",
    "    labels = pred.label_ids\n",
    "\n",
    "    # Tokenizer ile tahmin ve etiketleri metne dönüştür\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE Skorları\n",
    "    rouge_results = rouge_metric.compute(predictions=decoded_preds, \n",
    "                                         references=decoded_labels,\n",
    "                                         use_aggregator=True,\n",
    "                                         use_stemmer=True,\n",
    "                                        )\n",
    "    rouge1 = rouge_results[\"rouge1\"].mid.fmeasure\n",
    "    rouge2 = rouge_results[\"rouge2\"].mid.fmeasure\n",
    "    rougel = rouge_results[\"rougeL\"].mid.fmeasure\n",
    "    \n",
    "    # \n",
    "    # # FKGL Hesaplama\n",
    "    # def fkgl_score(text):\n",
    "    #     words = text.split()\n",
    "    #     sentences = text.count('.') + text.count('!') + text.count('?')\n",
    "    #     syllables = sum([sum(1 for char in word if char.lower() in \"aeiou\") for word in words])\n",
    "    #     if len(words) == 0 or sentences == 0:\n",
    "    #         return np.nan  # Bölüm sıfır hatasını önle\n",
    "    #     return 0.39 * (len(words) / sentences) + 11.8 * (syllables / len(words)) - 15.59\n",
    "    # \n",
    "    # fkgl_scores = [fkgl_score(pred) for pred in decoded_preds]\n",
    "    # fkgl = np.mean(fkgl_scores)\n",
    "    # \n",
    "    # # DCRS Hesaplama\n",
    "    # def dcrs(pred_text, ref_text):\n",
    "    #     scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    #     scores = scorer.score(ref_text, pred_text)\n",
    "    #     return (scores[\"rouge1\"].fmeasure + scores[\"rouge2\"].fmeasure + scores[\"rougeL\"].fmeasure) / 3\n",
    "    # \n",
    "    # dcrs_scores = [dcrs(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]\n",
    "    # dcrs = np.mean(dcrs_scores)\n",
    "    # \n",
    "    # # BARTScore Hesaplama\n",
    "    # def bartscore(pred_text, ref_text):\n",
    "    #     inputs = bart_tokenizer(ref_text, return_tensors=\"pt\", truncation=True, max_length=1024)\n",
    "    #     with torch.no_grad():\n",
    "    #         outputs = bart_model(**inputs, labels=bart_tokenizer(pred_text, return_tensors=\"pt\").input_ids)\n",
    "    #     return -outputs.loss.item()  # Negatif loss, yüksek skor daha iyi\n",
    "    # \n",
    "    # bart_scores = [bartscore(pred, ref) for pred, ref in zip(decoded_preds, decoded_labels)]\n",
    "    # bartscore_mean = np.mean(bart_scores)\n",
    "    # \n",
    "    return {\n",
    "        \"rouge1\": rouge1,\n",
    "        \"rouge2\": rouge2,\n",
    "        \"rougeL\": rougel,\n",
    "        # \"fkgl\": fkgl,\n",
    "        # \"dcrs\": dcrs,\n",
    "        # \"bartscore\": bartscore_mean,\n",
    "    }\n"
   ],
   "id": "8f96fbb38d8e9ab8",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:49.186736Z",
     "start_time": "2024-12-03T14:11:40.899159Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_train = pd.read_json(f'src/dataset/clean/{dataset_name}/{bert_version}_train.json')\n",
    "data_val = pd.read_json(f'src/dataset/clean/{dataset_name}/{bert_version}_validation.json')\n",
    "data_test = pd.read_json(f'src/dataset/clean/{dataset_name}/{bert_version}_test.json')"
   ],
   "id": "d8247cf4186b8fc5",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:49.203107Z",
     "start_time": "2024-12-03T14:11:49.193401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(len(data_train), len(data_val), len(data_test))\n",
    "data_train, data_val, data_test = data_val, data_test, data_train\n",
    "print(len(data_train), len(data_val), len(data_test))"
   ],
   "id": "7dfb098f3faaf9ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 241 3\n",
      "241 3 5\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "data_val = data_val.iloc[:2]\n",
    "print(len(data_train), len(data_val), len(data_test))"
   ],
   "id": "1cfecf36e55489c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:50.912186Z",
     "start_time": "2024-12-03T14:11:49.204403Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_train['rag_sentences'] = data_train['sentences_similarity'].apply(RAG)\n",
    "data_val['rag_sentences'] = data_val['sentences_similarity'].apply(RAG)\n",
    "data_test['rag_sentences'] = data_test['sentences_similarity'].apply(RAG)"
   ],
   "id": "7d518013220722d6",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:58.142328Z",
     "start_time": "2024-12-03T14:11:50.915474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data_train),\n",
    "    \"validation\": Dataset.from_pandas(data_val),\n",
    "    \"test\": Dataset.from_pandas(data_test)\n",
    "})"
   ],
   "id": "27d644a4dbafacd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:11:58.163418Z",
     "start_time": "2024-12-03T14:11:58.150350Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=8,  # Reduced rank\n",
    "    lora_alpha=8,  # Lower scaling factor\n",
    "    target_modules=[\"q\"],  # Update fewer modules (e.g., only query weights)\n",
    "    lora_dropout=0.1,  # Increased dropout for better regularization\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Task type remains the same\n",
    ")"
   ],
   "id": "3a5d12a2a8a4c2fc",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.779381Z",
     "start_time": "2024-12-03T14:11:58.164572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(genai_model_name, \n",
    "                                              torch_dtype=torch.bfloat16,\n",
    "                                              low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(genai_model_name)"
   ],
   "id": "e0b2dfb91d4f6f0c",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.785401Z",
     "start_time": "2024-12-03T14:12:08.781523Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ],
   "id": "9f466f2e2f8ce595",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.789947Z",
     "start_time": "2024-12-03T14:12:08.787432Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n"
   ],
   "id": "10db5795986a6c4c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.793650Z",
     "start_time": "2024-12-03T14:12:08.790639Z"
    }
   },
   "cell_type": "code",
   "source": "print(print_number_of_trainable_model_parameters(model))",
   "id": "5b04330f7f285d3f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 76961152\n",
      "all model parameters: 76961152\n",
      "percentage of trainable model parameters: 100.00%\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.852956Z",
     "start_time": "2024-12-03T14:12:08.794811Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model = get_peft_model(model, lora_config)",
   "id": "b213d957056f1204",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagatay/Library/Caches/pypoetry/virtualenvs/biolaysumm-biozu-ucOCEzNC-py3.12/lib/python3.12/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.856735Z",
     "start_time": "2024-12-03T14:12:08.853771Z"
    }
   },
   "cell_type": "code",
   "source": "print(print_number_of_trainable_model_parameters(peft_model))",
   "id": "ebfca73b3f87b293",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 172032\n",
      "all model parameters: 77133184\n",
      "percentage of trainable model parameters: 0.22%\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:08.866713Z",
     "start_time": "2024-12-03T14:12:08.857297Z"
    }
   },
   "cell_type": "code",
   "source": "sum([len(\" \".join(x).split()) for x in data_train['summary']]) / len(data_train['summary'])",
   "id": "fdbb835beaf89ea3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "389.8755186721992"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:16.872922Z",
     "start_time": "2024-12-03T14:12:16.861845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "total_x = []\n",
    "def tokenize_function(example):\n",
    "    global total_x\n",
    "    # Lay summary için optimize edilmiş prompt\n",
    "    prompt = prompt_factory(prompt_strategy_used, example)\n",
    "    summary = ' '.join(map(str, example['summary']))\n",
    "\n",
    "    # Girdileri tokenlaştırma\n",
    "    example['input_ids'] = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # T5-small için maksimum girdi boyutu\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.squeeze(0)\n",
    "\n",
    "    # Çıkışları (lay summary) tokenlaştırma\n",
    "    example['labels'] = tokenizer(\n",
    "        summary,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Lay summary genellikle kısa tutulur\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.squeeze(0)\n",
    "    \n",
    "    total_x.append(len(prompt.split()))\n",
    "    \n",
    "    return example\n"
   ],
   "id": "55f0e9deb613c83",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:23.999865Z",
     "start_time": "2024-12-03T14:12:19.092864Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'year', 'title', 'sections', 'headings', 'abstract', 'summary', 'keywords', 'sentences_similarity', 'rag_sentences'])"
   ],
   "id": "93dfd5d5e593a9da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/241 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc2912a4fbe543589c2d01d99831d20a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48b93fa7710c444290e868a141948156"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "726ebaff46ca4d6ba06eff194cf7c3ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:24.007151Z",
     "start_time": "2024-12-03T14:12:24.002084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for x in tokenized_datasets['validation']['input_ids']:\n",
    "    print(len(x))"
   ],
   "id": "68b85d22253f5abb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "1024\n",
      "1024\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:24.010889Z",
     "start_time": "2024-12-03T14:12:24.008921Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "print(tokenized_datasets)"
   ],
   "id": "ff83552b1ac00af4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (241, 2)\n",
      "Validation: (3, 2)\n",
      "Test: (5, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 241\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-03T14:12:45.002925Z",
     "start_time": "2024-12-03T14:12:35.644762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_dir = f'./results/BiOzU_{bert_version}_{dataset_name}_{prompt_strategy_used}-training'\n",
    "\n",
    "\n",
    "peft_training_args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    max_seq_length=1024,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # per_device_eval_batch_size=3,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    # eval_strategy=\"epoch\",\n",
    "    optim=\"adamw_hf\",\n",
    "    #optim=\"adamw_8bit\",\n",
    "    bf16=True,\n",
    "    )\n",
    "peft_trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    \n",
    "    \n",
    "    )"
   ],
   "id": "7fe93aa2bb7c7bb7",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-03T14:14:02.908936Z"
    }
   },
   "cell_type": "code",
   "source": "peft_trainer.train()",
   "id": "a108cdf9d99bb636",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cagatay/Library/Caches/pypoetry/virtualenvs/biolaysumm-biozu-ucOCEzNC-py3.12/lib/python3.12/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "peft_model_path=f\"./results/BiOzU_{bert_version}_{dataset_name}_{prompt_strategy_used}-checkpoint-local\"\n",
    "peft_trainer.model.save_pretrained(peft_model_path)\n",
    "tokenizer.save_pretrained(peft_model_path)"
   ],
   "id": "d093ebaf8d2852c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c6e0f372d91a1e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
