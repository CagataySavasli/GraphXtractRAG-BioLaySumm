{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:56.322354Z",
     "start_time": "2024-11-23T21:38:53.293185Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset\n",
    "from src.RAG_Calculater import RAG\n",
    "from src.Prompt_Factory import prompt_factory"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:56.462695Z",
     "start_time": "2024-11-23T21:38:56.323376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_name = \"elife\"\n",
    "\n",
    "data_train = pd.read_json(f'src/dataset/clean/{dataset_name}/train.json')\n",
    "data_val = pd.read_json(f'src/dataset/clean/{dataset_name}/validation.json')\n",
    "data_test = pd.read_json(f'src/dataset/clean/{dataset_name}/test.json')"
   ],
   "id": "daef21031d520e99",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:56.480245Z",
     "start_time": "2024-11-23T21:38:56.463393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_train['rag_sentences'] = data_train['sentences_similarity'].apply(RAG)\n",
    "data_val['rag_sentences'] = data_val['sentences_similarity'].apply(RAG)\n",
    "data_test['rag_sentences'] = data_test['sentences_similarity'].apply(RAG)"
   ],
   "id": "56d39647c58841da",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:56.612301Z",
     "start_time": "2024-11-23T21:38:56.480995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(data_train),\n",
    "    \"validation\": Dataset.from_pandas(data_val),\n",
    "    \"test\": Dataset.from_pandas(data_test)\n",
    "})"
   ],
   "id": "d37f884683091266",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:56.615511Z",
     "start_time": "2024-11-23T21:38:56.613783Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model_path=f\"./results/BiOzU_{dataset_name}-checkpoint-local\"",
   "id": "735da052467e48e9",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:58.585420Z",
     "start_time": "2024-11-23T21:38:56.616283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\", torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       peft_model_path,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False)"
   ],
   "id": "97fe6a865b0960df",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:38:58.589214Z",
     "start_time": "2024-11-23T21:38:58.586246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def tokenize_function(example):\n",
    "    # Lay summary için optimize edilmiş prompt\n",
    "    prompt = prompt_factory(1, example)\n",
    "    summary = ' '.join(map(str, example['summary']))\n",
    "\n",
    "    # Girdileri tokenlaştırma\n",
    "    example['input_ids'] = tokenizer(\n",
    "        prompt,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=1024,  # T5-small için maksimum girdi boyutu\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    # Çıkışları (lay summary) tokenlaştırma\n",
    "    example['labels'] = tokenizer(\n",
    "        summary,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # Lay summary genellikle kısa tutulur\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids\n",
    "\n",
    "    return example\n"
   ],
   "id": "f8f804b624af7a93",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:47:00.188048Z",
     "start_time": "2024-11-23T21:46:59.995302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenized_datasets = dataset.map(tokenize_function)\n",
    "tokenized_datasets = tokenized_datasets.remove_columns(['id', 'year', 'title', 'sections', 'headings', 'abstract', 'keywords', 'sentences_similarity', 'rag_sentences'])"
   ],
   "id": "562a55992cb14b18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5341b9f3e13c44f1a34cf4cb48f7c2a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5b49db083b8d46bfa16fb1c8ed730f54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "35e33e7346a749e092377e63e86ad7e5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:47:01.435333Z",
     "start_time": "2024-11-23T21:47:01.431520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#tokenized_datasets = tokenized_datasets.filter(lambda example, index: index % 100 == 0, with_indices=True)\n",
    "\n",
    "print(f\"Shapes of the datasets:\")\n",
    "print(f\"Training: {tokenized_datasets['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_datasets['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_datasets['test'].shape}\")\n",
    "print(tokenized_datasets)"
   ],
   "id": "8e5a2749c8cbfa62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of the datasets:\n",
      "Training: (5, 3)\n",
      "Validation: (2, 3)\n",
      "Test: (3, 3)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['summary', 'input_ids', 'labels'],\n",
      "        num_rows: 5\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['summary', 'input_ids', 'labels'],\n",
      "        num_rows: 2\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['summary', 'input_ids', 'labels'],\n",
      "        num_rows: 3\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:48:54.210683Z",
     "start_time": "2024-11-23T21:48:43.936980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_ids_list = tokenized_datasets['test']['input_ids']\n",
    "human_baseline_summaries = tokenized_datasets['test']['summary']\n",
    "peft_model_summaries = []\n",
    "\n",
    "for idx in tqdm(range(len(input_ids_list))):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = torch.tensor(input_ids_list[idx])\n",
    "    human_baseline_text_output = ' '.join(map(str, human_baseline_summaries[idx]))\n",
    "    human_baseline_summaries[idx] = human_baseline_text_output\n",
    "    peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=512, num_beams=1))\n",
    "    peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "    peft_model_summaries.append(peft_model_text_output)"
   ],
   "id": "3db96fc6d04b1d59",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:10<00:00,  3.42s/it]\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:48:54.220203Z",
     "start_time": "2024-11-23T21:48:54.214185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_results = pd.DataFrame({\n",
    "    'reference': human_baseline_summaries,\n",
    "    'prediction': peft_model_summaries\n",
    "})"
   ],
   "id": "f44522bbb75cfaea",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-23T21:48:54.226921Z",
     "start_time": "2024-11-23T21:48:54.221414Z"
    }
   },
   "cell_type": "code",
   "source": "model_results.to_csv(f'results/peft_model_summaries_{dataset_name}.csv')",
   "id": "bd1676f40696990e",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "168bc7ae2a307281"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
